# ----------------------------
# LLM Configuration
# ----------------------------
LLM_PROVIDER=ollama        # ollama | openai | azure
OLLAMA_MODEL=llama3        # Local Ollama model name

# Optional OpenAI / Azure (mocked for free)
OPENAI_API_KEY=
OPENAI_MODEL=
AZURE_OPENAI_KEY=
AZURE_OPENAI_ENDPOINT=
AZURE_OPENAI_DEPLOYMENT=
AZURE_OPENAI_VERSION=

# ----------------------------
# Data / Vector Store
# ----------------------------
DATA_DIR=data/raw          # Path to your documents
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
FAISS_INDEX_PATH=data/processed/faiss_index

# Pinecone (optional, demo or future integration)
PINECONE_API_KEY=
PINECONE_ENVIRONMENT=
PINECONE_INDEX_NAME=
USE_PINECONE=False        # True/False switch between FAISS and Pinecone

# ----------------------------
# Server / FastAPI
# ----------------------------
HOST=127.0.0.1
PORT=8000

# ----------------------------
# Misc
# ----------------------------
RETRY_COUNT=3              # Retry attempts for LLM
TIMEOUT_SECONDS=20         # Timeout per LLM call
CIRCUIT_BREAKER_THRESHOLD=3
CIRCUIT_BREAKER_RECOVERY=30  # seconds
